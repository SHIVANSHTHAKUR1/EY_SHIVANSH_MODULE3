{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Lab 2.3: Model Architecture Comparison\n",
    "**Module 3: Computer Vision and Image Processing**\n",
    "B-Tech AI Specialization | Chitkara University | February 2026\n",
    "\n",
    "---\n",
    "\n",
    "## üçì Industry Scenario\n",
    "> A client wants to deploy a classifier on a **Raspberry Pi** ‚Äî limited CPU, no GPU. You need to recommend which model architecture to use. The choice requires balancing accuracy with inference speed and memory. You need **real benchmark data** to make the recommendation.\n",
    "\n",
    "## üéØ Objective\n",
    "Benchmark **VGG16**, **ResNet50**, and **MobileNetV2** on inference speed and model size. Recommend which to use for edge deployment.\n",
    "\n",
    "**Time:** 60 minutes | **Mode:** Individual\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup ‚Äî Run First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running outside Colab; skipping custom widget manager setup.\n",
      "TensorFlow: 2.17.0\n",
      "GPU: []\n",
      "‚úÖ Ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Running outside Colab; skipping custom widget manager setup.\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Code\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"‚úÖ Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reveal_button() ready ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "def reveal_button(hint_text, solution_code):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, Code\n",
    "    out = widgets.Output()\n",
    "    hint_btn = widgets.Button(description='üí° Hint', button_style='info',\n",
    "        layout=widgets.Layout(width='120px', margin='4px'))\n",
    "    sol_btn  = widgets.Button(description='‚úÖ Solution', button_style='warning',\n",
    "        layout=widgets.Layout(width='140px', margin='4px'))\n",
    "    hide_btn = widgets.Button(description='üôà Hide', button_style='',\n",
    "        layout=widgets.Layout(width='100px', margin='4px'))\n",
    "    def on_hint(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background:#e3f2fd;padding:12px;border-radius:6px;'\n",
    "                f'border-left:4px solid #1976D2;font-size:14px\"><b>üí° Hint:</b><br>{hint_text}</div>'))\n",
    "    def on_sol(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            display(HTML('<b>‚úÖ Solution:</b>'))\n",
    "            display(Code(solution_code, language='python'))\n",
    "    def on_hide(b):\n",
    "        with out: out.clear_output()\n",
    "    hint_btn.on_click(on_hint); sol_btn.on_click(on_sol); hide_btn.on_click(on_hide)\n",
    "    display(widgets.HBox([hint_btn, sol_btn, hide_btn]), out)\n",
    "\n",
    "print(\"reveal_button() ready ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§î Predict Before You Benchmark\n",
    "\n",
    "Before writing any code, fill in your guesses in the table below. Commit to a number ‚Äî we'll compare against real results.\n",
    "\n",
    "| Model | Your guess: # Parameters | Your guess: Inference time (ms) | Best for? |\n",
    "|---|---|---|---|\n",
    "| VGG16 | ? M | ? ms | ? |\n",
    "| ResNet50 | ? M | ? ms | ? |\n",
    "| MobileNetV2 | ? M | ? ms | ? |\n",
    "\n",
    "Also answer:\n",
    "1. What is a \"residual connection\" (ResNet's key innovation)?\n",
    "2. What makes MobileNet \"mobile\" ‚Äî what did the designers sacrifice?\n",
    "3. On a Raspberry Pi with no GPU, does model size or architecture matter more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your predictions:\n",
    "## VGG16:       params ~138M,  latency ~90 ms  | best for accuracy when size doesn't matter.\n",
    "## ResNet50:    params ~25M,   latency ~55 ms  | best balanced option for accuracy vs cost.\n",
    "## MobileNetV2: params ~3.5M,  latency ~15 ms  | best for constrained CPU/edge deployments.\n",
    "\n",
    "## 1. Residual connections are identity shortcuts that allow gradients to flow directly across stacked blocks so deep nets avoid vanishing gradients.\n",
    "## 2. MobileNet is \"mobile\" because it uses depthwise separable convolutions to shrink compute/weights at the expense of raw accuracy capacity.\n",
    "## 3. On CPU, architecture matters (depthwise ops vs dense convs) because it governs FLOPs, though parameter size correlates with memory footprint too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Load All 3 Models\n",
    "\n",
    "Load each model with ImageNet weights. We use `include_top=True` so we get the full model including the classifier head ‚Äî this gives us realistic parameter counts.\n",
    "\n",
    "> ‚ö†Ô∏è This will download ~700 MB total. It takes 2‚Äì3 minutes on Colab ‚Äî that's expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m102967424/102967424\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\u001b[1m14536120/14536120\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "VGG16          :  138,357,544 parameters\n",
      "ResNet50       :   25,636,712 parameters\n",
      "MobileNetV2    :    3,538,984 parameters\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load all 3 models with imagenet weights\n",
    "# VGG16 expects 224x224, ResNet50 expects 224x224, MobileNetV2 expects 224x224\n",
    "\n",
    "vgg16       = VGG16(weights='imagenet', include_top=True)\n",
    "resnet50    = ResNet50(weights='imagenet', include_top=True)\n",
    "mobilenetv2 = MobileNetV2(weights='imagenet', include_top=True)\n",
    "\n",
    "models_dict = {\n",
    "    'VGG16':       vgg16,\n",
    "    'ResNet50':    resnet50,\n",
    "    'MobileNetV2': mobilenetv2,\n",
    "}\n",
    "\n",
    "# Quick sanity check ‚Äî print total params for each\n",
    "for name, model in models_dict.items():\n",
    "    if model is not None:\n",
    "        print(f\"{name:<15}: {model.count_params():>12,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"All three use the same pattern: <code>ModelName(weights='imagenet', include_top=True)</code>. \"\n",
    "              \"MobileNetV2 input defaults to 224x224 so no need to specify input_shape.\",\n",
    "    solution_code=(\n",
    "        \"vgg16       = VGG16(weights='imagenet', include_top=True)\\n\"\n",
    "        \"resnet50    = ResNet50(weights='imagenet', include_top=True)\\n\"\n",
    "        \"mobilenetv2 = MobileNetV2(weights='imagenet', include_top=True)\\n\\n\"\n",
    "        \"models_dict = {'VGG16': vgg16, 'ResNet50': resnet50, 'MobileNetV2': mobilenetv2}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Measure Inference Latency\n",
    "\n",
    "Run **100 inference passes** on a dummy image for each model. Average them to get stable latency numbers.\n",
    "\n",
    "> üí° **Why 100 passes?** The first few calls are slower (GPU/memory warmup). Averaging over 100 gives a stable, representative number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16: 175.1 ms/inference  (5.7 FPS)  |  138.4M params\n",
      "ResNet50: 105.6 ms/inference  (9.5 FPS)  |  25.6M params\n",
      "MobileNetV2: 70.4 ms/inference  (14.2 FPS)  |  3.5M params\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input image ‚Äî same shape that all 3 models expect\n",
    "dummy_input = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "N_RUNS = 100\n",
    "results = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    if model is None:\n",
    "        print(f\"‚ö†Ô∏è  {name} not loaded ‚Äî skipping\")\n",
    "        continue\n",
    "\n",
    "    # TODO: Warm up the model (run once before timing)\n",
    "    model.predict(dummy_input, verbose=0)\n",
    "\n",
    "    # TODO: Time N_RUNS inference passes\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(N_RUNS):\n",
    "        model.predict(dummy_input, verbose=0)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    avg_ms  = (elapsed / N_RUNS) * 1000\n",
    "    fps     = 1000 / avg_ms if avg_ms > 0 else 0\n",
    "    params  = model.count_params()\n",
    "\n",
    "    results[name] = {\n",
    "        'Parameters (M)':  round(params / 1e6, 1),\n",
    "        'Latency (ms)':    round(avg_ms, 1),\n",
    "        'FPS':             round(fps, 1),\n",
    "    }\n",
    "    print(f\"{name}: {avg_ms:.1f} ms/inference  ({fps:.1f} FPS)  |  {params/1e6:.1f}M params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"Call <code>model.predict(dummy_input, verbose=0)</code> once before the loop (warmup). \"\n",
    "              \"Then use <code>time.time()</code> before and after the loop to measure total elapsed time. \"\n",
    "              \"Average: <code>elapsed / N_RUNS * 1000</code> gives ms per inference.\",\n",
    "    solution_code=(\n",
    "        \"for name, model in models_dict.items():\\n\"\n",
    "        \"    model.predict(dummy_input, verbose=0)  # warmup\\n\"\n",
    "        \"    start = time.time()\\n\"\n",
    "        \"    for _ in range(N_RUNS):\\n\"\n",
    "        \"        model.predict(dummy_input, verbose=0)\\n\"\n",
    "        \"    elapsed = time.time() - start\\n\"\n",
    "        \"    avg_ms = (elapsed / N_RUNS) * 1000\\n\"\n",
    "        \"    fps    = 1000 / avg_ms\\n\"\n",
    "        \"    params = model.count_params()\\n\"\n",
    "        \"    results[name] = {\\n\"\n",
    "        \"        'Parameters (M)': round(params/1e6, 1),\\n\"\n",
    "        \"        'Latency (ms)':   round(avg_ms, 1),\\n\"\n",
    "        \"        'FPS':            round(fps, 1),\\n\"\n",
    "        \"    }\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Build & Display the Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_80889_row2_col0, #T_80889_row2_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "#T_80889_row2_col2 {\n",
       "  background-color: #ffcccc;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_80889\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_80889_level0_col0\" class=\"col_heading level0 col0\" >Parameters (M)</th>\n",
       "      <th id=\"T_80889_level0_col1\" class=\"col_heading level0 col1\" >Latency (ms)</th>\n",
       "      <th id=\"T_80889_level0_col2\" class=\"col_heading level0 col2\" >FPS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_80889_level0_row0\" class=\"row_heading level0 row0\" >VGG16</th>\n",
       "      <td id=\"T_80889_row0_col0\" class=\"data row0 col0\" >138.4</td>\n",
       "      <td id=\"T_80889_row0_col1\" class=\"data row0 col1\" >175.1</td>\n",
       "      <td id=\"T_80889_row0_col2\" class=\"data row0 col2\" >5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80889_level0_row1\" class=\"row_heading level0 row1\" >ResNet50</th>\n",
       "      <td id=\"T_80889_row1_col0\" class=\"data row1 col0\" >25.6</td>\n",
       "      <td id=\"T_80889_row1_col1\" class=\"data row1 col1\" >105.6</td>\n",
       "      <td id=\"T_80889_row1_col2\" class=\"data row1 col2\" >9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80889_level0_row2\" class=\"row_heading level0 row2\" >MobileNetV2</th>\n",
       "      <td id=\"T_80889_row2_col0\" class=\"data row2 col0\" >3.5</td>\n",
       "      <td id=\"T_80889_row2_col1\" class=\"data row2 col1\" >70.4</td>\n",
       "      <td id=\"T_80889_row2_col2\" class=\"data row2 col2\" >14.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2587d5a94c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create a pandas DataFrame from the results dict and display it\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "df.index.name = 'Model'\n",
    "\n",
    "styled = (df.style\n",
    "          .highlight_min(axis=0, color='lightgreen', subset=['Parameters (M)', 'Latency (ms)'])\n",
    "          .highlight_max(axis=0, color='#ffcccc', subset=['FPS'])\n",
    "          .format({'Parameters (M)': '{:.1f}', 'Latency (ms)': '{:.1f}', 'FPS': '{:.1f}'})\n",
    "         )\n",
    "\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal_button(\n",
    "    hint_text=\"<code>pd.DataFrame(results).T</code> transposes the dict into rows=models, cols=metrics. \"\n",
    "              \"Then <code>.style.highlight_min(axis=0, color='lightgreen')</code> highlights the best per column.\",\n",
    "    solution_code=(\n",
    "        \"df = pd.DataFrame(results).T\\n\"\n",
    "        \"df.index.name = 'Model'\\n\"\n",
    "        \"display(df.style.highlight_min(axis=0, color='lightgreen')\\n\"\n",
    "        \"           .highlight_max(axis=0, color='#ffcccc')\\n\"\n",
    "        \"           .format(precision=1))\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéöÔ∏è Task 5: Interactive Benchmark Explorer\n",
    "\n",
    "Use the controls to explore different views of the benchmark data. Which model is best depends on what you optimise for ‚Äî use this to build your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10157c3f5c0647f5b60fe6220677a301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='X axis:', layout=Layout(width='250px'), options=(('Paramet‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = widgets.Dropdown(\n",
    "    options=[('Parameters (M)', 'Parameters (M)'), ('Latency (ms)', 'Latency (ms)'), ('FPS', 'FPS')],\n",
    "    value='Parameters (M)', description='X axis:', layout=widgets.Layout(width='250px')\n",
    ")\n",
    "y_axis = widgets.Dropdown(\n",
    "    options=[('Latency (ms)', 'Latency (ms)'), ('FPS', 'FPS'), ('Parameters (M)', 'Parameters (M)')],\n",
    "    value='Latency (ms)', description='Y axis:', layout=widgets.Layout(width='250px')\n",
    ")\n",
    "chart_type = widgets.ToggleButtons(\n",
    "    options=['Scatter', 'Bar'], description='Chart:', button_style='info'\n",
    ")\n",
    "out_chart = widgets.Output()\n",
    "\n",
    "COLORS = {'VGG16': '#e74c3c', 'ResNet50': '#3498db', 'MobileNetV2': '#2ecc71'}\n",
    "\n",
    "def update_chart(change=None):\n",
    "    with out_chart:\n",
    "        out_chart.clear_output(wait=True)\n",
    "        if not results:\n",
    "            print(\"‚ö†Ô∏è  Run Tasks 2 and 3 first to populate results.\")\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "        if chart_type.value == 'Scatter':\n",
    "            for model_name, vals in results.items():\n",
    "                ax.scatter(vals[x_axis.value], vals[y_axis.value],\n",
    "                           s=200, color=COLORS[model_name], label=model_name, zorder=5)\n",
    "                ax.annotate(f\"  {model_name}\",\n",
    "                            (vals[x_axis.value], vals[y_axis.value]),\n",
    "                            fontsize=11, fontweight='bold', color=COLORS[model_name])\n",
    "            ax.set_xlabel(x_axis.value, fontsize=12)\n",
    "            ax.set_ylabel(y_axis.value, fontsize=12)\n",
    "        else:\n",
    "            model_names = list(results.keys())\n",
    "            vals = [results[m][y_axis.value] for m in model_names]\n",
    "            bars = ax.bar(model_names, vals, color=[COLORS[m] for m in model_names], width=0.5)\n",
    "            for bar, val in zip(bars, vals):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(vals)*0.01,\n",
    "                        f'{val}', ha='center', fontsize=11, fontweight='bold')\n",
    "            ax.set_ylabel(y_axis.value, fontsize=12)\n",
    "\n",
    "        ax.set_title(f'Model Comparison: {y_axis.value}', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        if chart_type.value == 'Scatter': ax.legend(fontsize=11)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "x_axis.observe(update_chart, names='value')\n",
    "y_axis.observe(update_chart, names='value')\n",
    "chart_type.observe(update_chart, names='value')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([x_axis, y_axis, chart_type]),\n",
    "    out_chart\n",
    "]))\n",
    "update_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úçÔ∏è Task 6: Your Recommendation\n",
    "\n",
    "Based on the benchmark data, write **3 sentences** recommending which model to deploy on the Raspberry Pi. Your answer should justify:\n",
    "- Why you chose that model (cite specific numbers)\n",
    "- What you're sacrificing (every choice has a trade-off)\n",
    "- One condition under which you'd choose a different model instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Your recommendation:\n",
    "## I recommend MobileNetV2 because it delivers 14.2 FPS with only 3.5M params, which keeps CPU latency (~70 ms) and RAM footprint low enough for Raspberry Pi.\n",
    "## The trade-off is giving up ~5% accuracy potential versus deeper nets; VGG16/ResNet50 have more representational capacity but are 7-40√ó heavier and >100 ms per inference.\n",
    "## I would switch to ResNet50 if the device had an NPU/GPU accelerator or if the business demanded ImageNet-level accuracy and could tolerate ~10 FPS throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§î Compare Against Your Predictions\n",
    "\n",
    "Go back to your predictions at the top. How close were you?\n",
    "\n",
    "| Model | Predicted params | Actual params | Predicted latency | Actual latency |\n",
    "|---|---|---|---|---|\n",
    "| VGG16 | ? | | ? | |\n",
    "| ResNet50 | ? | | ? | |\n",
    "| MobileNetV2 | ? | | ? | |\n",
    "\n",
    "**Which result surprised you the most? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è What surprised me most:\n",
    "## I was most wrong about how slow VGG16 really is; my guess of ~90 ms underestimated the observed 175 ms, highlighting how bandwidth-heavy 138M params become on CPU.\n",
    "## This makes sense because VGG stacks dense 3x3 convs without shortcuts or depthwise tricks, so FLOPs scale cubically and memory thrashing dominates on Pi-class hardware."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_tf (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
